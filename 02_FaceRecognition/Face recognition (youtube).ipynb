{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection/recognition in videos, using Amazon Rekognition\n",
    "\n",
    "- The idea of this notebook is to show how to use Amazon Rekognition images for detecting faces and recognizing these faces in videos.\n",
    "\n",
    "- So, we will:\n",
    "  - use **pytube** to download a given video from YouTube.\n",
    "  - use **OpenCV** for reading, transforming and writing the video for each step of transformation\n",
    "  \n",
    "## Steps\n",
    "1. Download the video from YouTube\n",
    "1. Extract a piece and a few frames (2/sec - 5mins) from the video\n",
    "1. For each frame, calls Amazon Rekognition face detect\n",
    "1. For each detected face, add it to a collection (index it)\n",
    "1. Manually name each face-identity \n",
    "1. Render a new video with the bounding boxes around the faces and their respective names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import boto3\n",
    "import botocore\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import threading\n",
    "import sys\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "from ipywidgets import FloatProgress, VBox, HTML\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not 'pytube' in sys.modules:\n",
    "    !pip install pytube\n",
    "\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions/objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder for converting numpy to json\n",
    "class NumPyArangeEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist() # or map(int, obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "    return reko.detect_faces(\n",
    "        Image={\n",
    "            'Bytes': bytearray( img )\n",
    "        },\n",
    "        Attributes=['ALL']\n",
    "    )[\"FaceDetails\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_faces(img):\n",
    "    resp = []\n",
    "    try:\n",
    "        resp = reko.search_faces_by_image(\n",
    "            CollectionId='face_detection',\n",
    "            Image={\n",
    "                \"Bytes\": bytearray(img)\n",
    "            },\n",
    "            MaxFaces=5,\n",
    "            FaceMatchThreshold=min_threshold\n",
    "        )[\"FaceMatches\"]\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_face(img):\n",
    "    try:\n",
    "        return reko.index_faces(\n",
    "            CollectionId='face_detection',\n",
    "            Image={\n",
    "                \"Bytes\": bytearray(img)\n",
    "            }\n",
    "        )[\"FaceRecords\"]\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def face_detection(frame,frame_id):\n",
    "    global faces\n",
    "    global consumers\n",
    "    global consumers_lock\n",
    "    global bar, label, short_max_frames\n",
    "    \n",
    "    h,w,_ = frame.shape\n",
    "    \n",
    "    img = cv2.imencode(\".jpg\", frame )[1]\n",
    "    resp = detect_faces(img)\n",
    "    \n",
    "    for i in resp:     \n",
    "        bbox = i['BoundingBox']\n",
    "        x1,y1 = ( int(bbox['Left'] * w), int(bbox['Top'] * h) )\n",
    "        x2,y2 = ( x1 + int(bbox['Width'] * w), y1 + int(bbox['Height'] * h) )\n",
    "        \n",
    "        face = frame[y1:y2, x1:x2]        \n",
    "        faces[frame_id] = [] if faces.get(frame_id) is None else faces[frame_id]\n",
    "        faces[frame_id].append({'face': face, 'bbox': (x1, y1, x2, y2)})\n",
    "        \n",
    "    consumers_lock.acquire()\n",
    "    consumers -= 1\n",
    "    bar.value += 100/(short_max_frames)\n",
    "    label.value = \"{}% Consumers[{}]\".format(int(bar.value), consumers)\n",
    "    consumers_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def face_indexing(face):\n",
    "    global consumers_lock, consumers\n",
    "    global bar, label, faces\n",
    "    \n",
    "    face_img = face['face']\n",
    "    \n",
    "    hf, wf, _ = face_img.shape\n",
    "    min_size = min(hf, wf)\n",
    "    if min_size < 90.0:\n",
    "        scale = 90.0 / min_size\n",
    "        face_img = cv2.resize(face_img, (0,0), fx=scale, fy=scale )\n",
    "\n",
    "    face_img_ = cv2.imencode(\".jpg\", face_img )[1]\n",
    "    resp = search_faces(face_img_)\n",
    "    \n",
    "    if len(resp) == 0:\n",
    "        resp = index_face(face_img_)\n",
    "        if len(resp) > 0:  \n",
    "            face_id = resp[0][\"Face\"][\"FaceId\"]\n",
    "            face['face_id'] = face_id\n",
    "            people.append(face_id)\n",
    "            cv2.imwrite(\"/tmp/face_%s.jpg\" % face_id, face['face'])\n",
    "    else:\n",
    "        face_id = resp[0][\"Face\"][\"FaceId\"]\n",
    "        face['face_id'] = face_id\n",
    "        \n",
    "    consumers_lock.acquire()\n",
    "    consumers -= 1\n",
    "    bar.value += 100/len(faces)\n",
    "    label.value = \"{}%\".format(int(bar.value))\n",
    "    consumers_lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render_video(file_name, w=640, h=360):\n",
    "    display(HTML('<video controls width=\"%s\" height=\"%s\"><source src=\"%s\" type=\"video/mp4\"/></video>' % (w,h,file_name) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_threshold=75 # confidence level for a face matching\n",
    "fps=30 # orignal video FPS\n",
    "duration=5 # max duration of the new video\n",
    "max_frames=fps * 60 * duration # max number of frames of the original video\n",
    "step=15 # step for getting the frames. i.e: get one, jump 15, get another, jump 15...\n",
    "new_fps=fps/step # new video fps\n",
    "file_name = \"video\" # name of the saved video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reko = boto3.client(\"rekognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(file_name):\n",
    "    stream = YouTube('https://www.youtube.com/watch?v=gobyhRLjp6Y').streams\n",
    "    stream.first().download(filename=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Destroy/Create the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    reko.delete_collection(CollectionId='face_detection')\n",
    "    reko.create_collection(\n",
    "        CollectionId='face_detection'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract frames into a new video (5mins 2fps)\n",
    "- Here, the original video will be converted in a short version with less duration and fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bar = FloatProgress(min=0, max=100)\n",
    "label = HTML(\"0%\")\n",
    "box = VBox(children=[label, bar])\n",
    "display(box)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'X264')\n",
    "out = cv2.VideoWriter('short-%s.mp4' % file_name, fourcc, new_fps, (640,360))\n",
    "\n",
    "cap = cv2.VideoCapture(\"%s.mp4\" % file_name)\n",
    "frame_counter = 0\n",
    "while(cap.isOpened()):\n",
    "    if frame_counter > max_frames:\n",
    "        break\n",
    "        \n",
    "    cap.set( cv2.CAP_PROP_POS_FRAMES, frame_counter )\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame, (640, 360) )\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_counter += step\n",
    "    \n",
    "    bar.value += 100/(max_frames/step)\n",
    "    label.value = \"{}%\".format(int(bar.value))\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_video(\"short-%s.mp4\" % file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Faces\n",
    "- Here, each frame will be sent to Amaon Rekognition for face detection. It is expected that the output be a list of faces and it's respective bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_consumers = 20 # number of workers that will run in parallel for face detection\n",
    "consumers = 0\n",
    "consumers_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "from ipywidgets import FloatProgress, VBox, HTML\n",
    "from IPython.display import display\n",
    "\n",
    "bar = FloatProgress(min=0, max=100)\n",
    "label = HTML(\"0%\")\n",
    "box = VBox(children=[label, bar])\n",
    "display(box)\n",
    "\n",
    "short_max_frames = new_fps * 60 * duration\n",
    "\n",
    "faces = {}\n",
    "frame_id = 0\n",
    "cap = cv2.VideoCapture(\"short-%s.mp4\" % file_name)\n",
    "while(cap.isOpened()):\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    while consumers > max_consumers:\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    consumers_lock.acquire()\n",
    "    consumers += 1\n",
    "    consumers_lock.release()\n",
    "    \n",
    "    consumer = threading.Thread(target=face_detection, args=(frame,frame_id,))\n",
    "    frame_id += 1\n",
    "    consumer.start()\n",
    "    \n",
    "while consumers > 0:\n",
    "    time.sleep(0.5)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index faces\n",
    "- Here, each face will be sent for face indexing. If a given face was already indexed it will be found in the collection and its face_id will be returned. Otherwise, the face is then indexed (added to the collection) and a new face_id is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -f /tmp/*.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_consumers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bar = FloatProgress(min=0, max=100)\n",
    "label = HTML(\"0%\")\n",
    "box = VBox(children=[label, bar])\n",
    "display(box)\n",
    "\n",
    "people=[]\n",
    "for frame_id in faces:\n",
    "    for i in faces[frame_id]:\n",
    "        \n",
    "        while consumers > max_consumers:\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        consumers_lock.acquire()\n",
    "        consumers += 1\n",
    "        consumers_lock.release()\n",
    "\n",
    "        consumer = threading.Thread(target=face_indexing, args=(i,))\n",
    "        consumer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face tagging\n",
    "- The faces found in the indexing process will be rendered below. You need to edit the dict returned and then load it for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "counter = 0\n",
    "\n",
    "fig=plt.figure(figsize=(8, 8))\n",
    "\n",
    "rows = 0\n",
    "cols = 0\n",
    "counter = 1\n",
    "\n",
    "database = \"people_name = {\\n\"\n",
    "\n",
    "for i in people:\n",
    "   \n",
    "    path = '/tmp/face_%s.jpg' % i\n",
    "    database += \"    '%s': 'Anon',\\n\" % i\n",
    "    #print(path)\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    cols = cols % 4\n",
    "    rows = rows % 8\n",
    "        \n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    fig.add_subplot(8, 4, counter)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    counter += 1\n",
    "database += \"}\\n\"\n",
    "print(database)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past here the structure above \"people_name\", edit it with the correct names and eval it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_name = {\n",
    "    '999bd618-c763-4a04-94f9-3c54094ef788': 'Gentili',\n",
    "    '9bc675f1-7d37-4efd-94d2-70ccdc6a474a': 'Gentili',\n",
    "    '7d32caa3-e8c0-447b-a1c3-37411b849cc4': 'Diguinho',\n",
    "    'cc3619d1-f7dc-48b8-b0d7-4af256b78029': 'Gentili',\n",
    "    '2503def9-614b-4323-bc43-25d8ae67cc8f': 'Batera',\n",
    "    'bfbdad31-48b2-48ec-a364-b19bdfcf1ff7': 'Roger',\n",
    "    'b10ac110-dfb9-42e1-8910-b5bce106eb19': 'Roger',\n",
    "    'a9b0c08c-50a1-4dc4-9253-10f080f53d6c': 'Anon1',\n",
    "    '338770b2-809c-4bf7-be3a-54d24414d400': 'Kojima',\n",
    "    '47245012-ddb3-4288-a0b0-97fe216a10ae': 'Anon1',\n",
    "    '890f511c-0756-4af5-bdad-88944993dd74': 'Anon1',\n",
    "    '44e1aa48-9c91-4d78-90c9-a59db9391b48': 'Gentili',\n",
    "    '8f01f5e8-3854-4ecd-8431-cfa0761b38a7': 'Kojima',\n",
    "    '0d43a0d0-903b-4e06-8210-59b7ae1be940': 'Kojima',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export parameters (optional)\n",
    "- you can save the metadata for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "f = open(\"faces.json\", \"w\")\n",
    "f.write(json.dumps(faces, cls=NumPyArangeEncoder) )\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"people_name.json\", \"w\")\n",
    "f.write(json.dumps(people_name) )\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import paramters (optional)\n",
    "- you can import the previous saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "people_name = json.loads( open(\"people_name.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faces = json.loads( open(\"faces.json\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process video\n",
    "- Finally we will get the short version of the video and all the metadata and render a new video with the information on top of its frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bar = FloatProgress(min=0, max=100)\n",
    "label = HTML(\"0%\")\n",
    "box = VBox(children=[label, bar])\n",
    "display(box)\n",
    "\n",
    "max_frames = len(faces)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'X264')\n",
    "out = cv2.VideoWriter('short-processed-%s.mp4' % file_name, fourcc, new_fps, (640,360))\n",
    "\n",
    "cap = cv2.VideoCapture('short-%s.mp4' % file_name)\n",
    "frame_counter = 0\n",
    "while(cap.isOpened()):\n",
    "    _, frame = cap.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    h,w,_ = frame.shape\n",
    "    thick = int((h + w) // 600)\n",
    "\n",
    "    if faces.get(frame_counter) is not None:\n",
    "        for i in faces[frame_counter]:\n",
    "            (x1,y1,x2,y2) = i['bbox']\n",
    "\n",
    "            face_id = i.get('face_id')\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255,255,255), thick)\n",
    "            if face_id is not None:\n",
    "                cv2.putText(frame, people_name[face_id], (x1, y1), 0, 1, (255,255,0), thick)\n",
    "\n",
    "    out.write(frame)\n",
    "    \n",
    "    frame_counter += 1\n",
    "    \n",
    "    bar.value += 100/max_frames\n",
    "    label.value = \"{}%\".format(int(bar.value))\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "render_video('short-processed-%s.mp4' % file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
