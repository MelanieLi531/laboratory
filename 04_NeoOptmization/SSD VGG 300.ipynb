{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SSD VGG NET](https://cdn-images-1.medium.com/max/1200/1*pPxrkm4Urz04Ez65mwWE9Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "    all its variables into constant \n",
    "    Args:\n",
    "        model_dir: the root folder containing the checkpoint state file\n",
    "        output_node_names: a string, containing all the output node's names, \n",
    "                            comma separated\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    if not output_node_names:\n",
    "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "        return -1\n",
    "\n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    # We precise the file fullname of our freezed graph\n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We start a session using a temporary fresh Graph\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # We import the meta graph in the current default Graph\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We restore the weights\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "        ) \n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "    return output_graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDVGG300(object):\n",
    "    \"\"\"SSD VGG 300 model\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        dropout_keep_prob=0.5\n",
    "        is_training=False\n",
    "        paddings = [[0, 0], [1, 1], [1, 1], [0, 0]]\n",
    "        feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n",
    "        anchor_size_bounds=[0.15, 0.90]\n",
    "        anchor_sizes=[(21., 45.),\n",
    "                      (45., 99.),\n",
    "                      (99., 153.),\n",
    "                      (153., 207.),\n",
    "                      (207., 261.),\n",
    "                      (261., 315.)]\n",
    "        anchor_ratios=[[2, .5],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5],\n",
    "                       [2, .5]]\n",
    "        anchor_steps=[8, 16, 32, 64, 100, 300]\n",
    "        anchor_offset=0.5\n",
    "        normalizations=[20, -1, -1, -1, -1, -1]\n",
    "        prior_scaling=[0.1, 0.1, 0.2, 0.2]\n",
    "        features = []\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, 300, 300, 3])\n",
    "        net = self.x\n",
    "        \n",
    "        net = tf.layers.conv2d(net, filters=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv1_1')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv1_2')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(1, 1), padding='SAME', name='pool1')\n",
    "\n",
    "        # conv2\n",
    "        net = tf.layers.conv2d(net, filters=128, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv2_1')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=128, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv2_2')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(1, 1), padding='SAME', name='pool2')\n",
    "\n",
    "        # conv3\n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv3_1')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv3_2')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv3_3')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(1, 1), padding='SAME', name='pool3')\n",
    "\n",
    "        # conv4\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv4_1')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv4_2')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv4_3')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        features.append(net)\n",
    "        \n",
    "        net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(1, 1), padding='SAME', name='pool4')\n",
    "\n",
    "        # conv5\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv5_1')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv5_2')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "\n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=(1, 1), padding='SAME', name='conv5_3')\n",
    "        #net = tf.layers.batch_normalization(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        net = tf.layers.max_pooling2d(net, pool_size=(3, 3), strides=1, name='pool5')\n",
    "        \n",
    "        # Additional SSD blocks.\n",
    "        # Block 6: let's dilate the hell out of it!\n",
    "        #net = tf.layers.conv2d(net, filters=1024, kernel_size=(3, 3), dilation_rate=6, name='conv6')\n",
    "        net = tf.layers.conv2d(net, filters=1024, kernel_size=(3, 3), name='conv6')\n",
    "        net = tf.layers.dropout(inputs=net, rate=dropout_keep_prob, training=is_training)\n",
    "        \n",
    "        # Block 7: 1x1 conv\n",
    "        net = tf.layers.conv2d(net, filters=1024, kernel_size=(1, 1), name='conv7')\n",
    "        features.append(net)\n",
    "        net = tf.layers.dropout(inputs=net, rate=dropout_keep_prob, training=is_training)\n",
    "        \n",
    "        # Block 8/9/10/11: 1x1 and 3x3 convolutions stride 2 (except lasts).\n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(1, 1), name='conv1x1_1')\n",
    "        net = tf.pad(net, paddings, \"CONSTANT\")       \n",
    "        net = tf.layers.conv2d(net, filters=512, kernel_size=(3, 3), strides=2, padding='VALID', name='conv3x3_1')\n",
    "        features.append(net)\n",
    "        \n",
    "        net = tf.layers.conv2d(net, filters=128, kernel_size=(1, 1), name='conv1x1_2')\n",
    "        net = tf.pad(net, paddings, \"CONSTANT\")\n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), strides=2, padding='VALID', name='conv3x3_2')\n",
    "        features.append(net)\n",
    "        \n",
    "        \n",
    "        net = tf.layers.conv2d(net, filters=128, kernel_size=(1, 1), name='conv1x1_3') \n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), padding='VALID', name='conv3x3_3')\n",
    "        features.append(net)\n",
    "        \n",
    "        net = tf.layers.conv2d(net, filters=128, kernel_size=(1, 1), name='conv1x1_4') \n",
    "        net = tf.layers.conv2d(net, filters=256, kernel_size=(3, 3), padding='VALID', name='conv3x3_4')\n",
    "        features.append(net)\n",
    "        \n",
    "        self.predictions, self.logits, self.localisations = [],[],[]\n",
    "        for i, layer in enumerate(features):\n",
    "            num_anchors = len(anchor_sizes[i]) + len(anchor_ratios[i])\n",
    "\n",
    "            # Location.\n",
    "            num_loc_pred = num_anchors * 4\n",
    "            loc_pred = tf.layers.conv2d(net, filters=num_loc_pred, kernel_size=(3, 3), name='conv_loc_%d' % i) \n",
    "            loc_new_shape = loc_pred.get_shape().as_list()[:-1]+[num_anchors, 4]\n",
    "            loc_new_shape[0] = -1\n",
    "            loc_pred = tf.reshape(loc_pred, loc_new_shape, name='location_%d' %i )\n",
    "            \n",
    "            # Class prediction.\n",
    "            num_cls_pred = num_anchors * num_classes\n",
    "            cls_pred = tf.layers.conv2d(net, filters=num_cls_pred, kernel_size=(3, 3), name='conv_cls_%d' % i)\n",
    "            cls_new_shape = cls_pred.get_shape().as_list()[:-1]+[num_anchors, num_classes]\n",
    "            cls_new_shape[0] = -1\n",
    "            cls_pred = tf.reshape(cls_pred, cls_new_shape, name='conf_%d' %i)\n",
    "        \n",
    "            self.predictions.append(tf.nn.softmax(cls_pred, name='class_%d' % i))\n",
    "            self.logits.append(cls_pred)\n",
    "            self.localisations.append(loc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSDVGG300(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "names = []\n",
    "for i in range(len(net.predictions)):\n",
    "    outputs[net.predictions[i].name] = net.predictions[i]\n",
    "    outputs[net.logits[i].name] = net.logits[i]\n",
    "    outputs[net.localisations[i].name] = net.localisations[i]\n",
    "    names.append(net.predictions[i].name[:-2])\n",
    "    names.append(net.logits[i].name[:-2])\n",
    "    names.append(net.localisations[i].name[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model/\n",
    "exporter = tf.saved_model.builder.SavedModelBuilder('model')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    " \n",
    "    exporter.add_meta_graph_and_variables(\n",
    "        sess, \n",
    "        tags=[tf.saved_model.tag_constants.SERVING], \n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "            tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "                inputs={\"inputs\": net.x}, \n",
    "                outputs=outputs\n",
    "            )\n",
    "        },\n",
    "        strip_default_attrs=True)\n",
    "    #exporter.save()\n",
    "    saver.save(sess, '/home/ec2-user/SageMaker/GTC2019/model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ec2-user/SageMaker/GTC2019/model/model\n",
      "INFO:tensorflow:Froze 70 variables.\n",
      "INFO:tensorflow:Converted 70 variables to const ops.\n",
      "265 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "_ = freeze_graph('/home/ec2-user/SageMaker/GTC2019/model', ','.join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_model.pb\r\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz && cd model && tar -czvf ../model.tar.gz frozen_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-715445047862\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Retrieve the default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-east-1-715445047862/neo/SSDVGG300/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "role='arn:aws:iam::715445047862:role/MachineLearningHost'\n",
    "job_prefix='SSDVGG300'\n",
    "path='neo/%s' % job_prefix\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "!aws s3 cp model.tar.gz s3://$default_bucket/$path/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'CompilationJobArn': u'arn:aws:sagemaker:us-east-1:715445047862:compilation-job/SSDVGG300-1552519318',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '101',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 13 Mar 2019 23:21:58 GMT',\n",
       "   'x-amzn-requestid': '92ae9cc1-4a0c-4857-a860-d43095ba9368'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': '92ae9cc1-4a0c-4857-a860-d43095ba9368',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name=\"%s-%d\" % (job_prefix, int(time.time()))\n",
    "sm.create_compilation_job(\n",
    "    CompilationJobName=job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': \"s3://%s/%s/model.tar.gz\" % (default_bucket, path),\n",
    "        'DataInputConfig': '{\"data\":[1,300,300,3]}',\n",
    "        'Framework': 'TENSORFLOW'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': \"s3://%s/%s/\" % (default_bucket, path),\n",
    "        'TargetDevice': 'ml_c5' #'ml_m4'|'ml_m5'|'ml_c4'|'ml_c5'|'ml_p2'|'ml_p3'|'jetson_tx1'|'jetson_tx2'|'rasp3b'|'deeplens'\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 300\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "imgMean = np.array([104, 117, 124], np.float)\n",
    "img = cv2.imread(\"TestImage.jpg\")\n",
    "img = cv2.resize(img.astype(float), (300, 300)) #resize\n",
    "img -= imgMean #subtract image mean\n",
    "img = img.reshape((1, 300, 300, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 14.4 ms, total: 179 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 7.42 s, total: 21 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = sess.run(list(outputs.values()), feed_dict = {net.x: img})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-715445047862/neo/SSDVGG300/model-ml_c5.tar.gz to ./model-ml_c5.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-715445047862/neo/SSDVGG300/model-ml_c5.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled.params\n",
      "compiled_model.json\n",
      "compiled.so\n"
     ]
    }
   ],
   "source": [
    "!rm -rf neo_test && mkdir neo_test\n",
    "!tar -xzvf model-ml_c5.tar.gz -C neo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-9.0\n",
      "-- Found CUDA_CUDA_LIBRARY=/usr/local/cuda-9.0/lib64/stubs/libcuda.so\n",
      "-- Found CUDA_CUDART_LIBRARY=/usr/local/cuda-9.0/lib64/libcudart.so\n",
      "-- Found CUDA_NVRTC_LIBRARY=/usr/local/cuda-9.0/lib64/libnvrtc.so\n",
      "-- Found CUDA_CUDNN_LIBRARY=/usr/local/cuda-9.0/lib64/libcudnn.so\n",
      "-- Found CUDA_CUBLAS_LIBRARY=/usr/local/cuda-9.0/lib64/libcublas.so\n",
      "-- Custom CUDA_PATH=/usr/local/cuda-9.0\n",
      "-- CUDA_CUDA_LIBRARY: /usr/local/cuda-9.0/lib64/stubs/libcuda.so\n",
      "-- CUDA_CUDART_LIBRARY: /usr/local/cuda-9.0/lib64/libcudart.so\n",
      "-- Custom CUDNN_PATH=OFF\n",
      "-- CUDA_CUDNN_LIBRARY: /usr/local/cuda-9.0/lib64/libcudnn.so\n",
      "-- Build with RPC support...\n",
      "-- Build with Graph runtime support...\n",
      "-- Build VTA runtime with target: sim\n",
      "-- Found CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-9.0\n",
      "-- Found CUDA_CUDA_LIBRARY=/usr/local/cuda-9.0/lib64/stubs/libcuda.so\n",
      "-- Found CUDA_CUDART_LIBRARY=/usr/local/cuda-9.0/lib64/libcudart.so\n",
      "-- Found CUDA_NVRTC_LIBRARY=/usr/local/cuda-9.0/lib64/libnvrtc.so\n",
      "-- Found CUDA_CUDNN_LIBRARY=/usr/local/cuda-9.0/lib64/libcudnn.so\n",
      "-- Found CUDA_CUBLAS_LIBRARY=/usr/local/cuda-9.0/lib64/libcublas.so\n",
      "-- Try OpenMP C flag = [-fopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [ ]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-fopenmp=libomp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [/openmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-Qopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-openmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-xopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [+Oopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-qsmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-mp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Could NOT find OpenMP (missing:  OpenMP_C_FLAGS) \n",
      "-- CMake version: 3.6.1\n",
      "-- Version: 4.1.0\n",
      "-- Build type: Release\n",
      "-- /home/ec2-user/SageMaker/GTC2019/neo-ai-dlr/3rdparty/treelite/dmlc-core/cmake/build_config.h.in -> /home/ec2-user/SageMaker/GTC2019/neo-ai-dlr/3rdparty/treelite/dmlc-core/include/dmlc/build_config.h\n",
      "-- Try OpenMP C flag = [-fopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [ ]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-fopenmp=libomp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [/openmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-Qopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-openmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-xopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [+Oopenmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-qsmp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Try OpenMP C flag = [-mp]\n",
      "-- Performing Test OpenMP_FLAG_DETECTED\n",
      "-- Performing Test OpenMP_FLAG_DETECTED - Failed\n",
      "-- Could NOT find OpenMP (missing:  OpenMP_C_FLAGS) \n",
      "-- TVM_RUNTIME_LINKER_LIBS: /usr/local/cuda-9.0/lib64/libcudart.so/usr/local/cuda-9.0/lib64/stubs/libcuda.so/usr/local/cuda-9.0/lib64/libnvrtc.so/usr/local/cuda-9.0/lib64/libcudnn.so\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ec2-user/SageMaker/GTC2019/neo-ai-dlr/build\n",
      "[ 13%] Built target objtreelite_runtime\n",
      "[ 26%] Built target objdlr\n",
      "[ 53%] Built target dmlc\n",
      "[ 93%] Built target tvm_runtime_static\n",
      "[100%] Built target treelite_runtime_static\n",
      "[100%] Built target dlr\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing dlr.egg-info/PKG-INFO\n",
      "writing dependency_links to dlr.egg-info/dependency_links.txt\n",
      "writing requirements to dlr.egg-info/requires.txt\n",
      "writing top-level names to dlr.egg-info/top_level.txt\n",
      "reading manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "writing manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/__init__.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/api.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/libpath.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/api.py to api.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/libpath.py to libpath.cpython-36.pyc\n",
      "installing package data to build/bdist.linux-x86_64/egg\n",
      "running install_data\n",
      "copying ../lib/libdlr.so -> build/bdist.linux-x86_64/egg/dlr\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/dlr-1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing dlr-1.0-py3.6.egg\n",
      "creating /home/ec2-user/.local/lib/python3.6/site-packages/dlr-1.0-py3.6.egg\n",
      "Extracting dlr-1.0-py3.6.egg to /home/ec2-user/.local/lib/python3.6/site-packages\n",
      "Adding dlr 1.0 to easy-install.pth file\n",
      "\n",
      "Installed /home/ec2-user/.local/lib/python3.6/site-packages/dlr-1.0-py3.6.egg\n",
      "Processing dependencies for dlr==1.0\n",
      "Searching for decorator==4.3.0\n",
      "Best match: decorator 4.3.0\n",
      "Adding decorator 4.3.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/lib/python3.6/site-packages\n",
      "Searching for numpy==1.14.5\n",
      "Best match: numpy 1.14.5\n",
      "Adding numpy 1.14.5 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/lib/python3.6/site-packages\n",
      "Finished processing dependencies for dlr==1.0\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing requirements to dlr.egg-info/requires.txt\n",
      "writing dlr.egg-info/PKG-INFO\n",
      "writing top-level names to dlr.egg-info/top_level.txt\n",
      "writing dependency_links to dlr.egg-info/dependency_links.txt\n",
      "reading manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "writing manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/__init__.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/api.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/libpath.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/__init__.py to __init__.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/api.py to api.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/libpath.py to libpath.pyc\n",
      "installing package data to build/bdist.linux-x86_64/egg\n",
      "running install_data\n",
      "copying ../lib/libdlr.so -> build/bdist.linux-x86_64/egg/dlr\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/dlr-1.0-py2.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing dlr-1.0-py2.7.egg\n",
      "removing '/home/ec2-user/.local/lib/python2.7/site-packages/dlr-1.0-py2.7.egg' (and everything under it)\n",
      "creating /home/ec2-user/.local/lib/python2.7/site-packages/dlr-1.0-py2.7.egg\n",
      "Extracting dlr-1.0-py2.7.egg to /home/ec2-user/.local/lib/python2.7/site-packages\n",
      "dlr 1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /home/ec2-user/.local/lib/python2.7/site-packages/dlr-1.0-py2.7.egg\n",
      "Processing dependencies for dlr==1.0\n",
      "Searching for decorator==4.3.0\n",
      "Best match: decorator 4.3.0\n",
      "Adding decorator 4.3.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages\n",
      "Searching for numpy==1.14.5\n",
      "Best match: numpy 1.14.5\n",
      "Adding numpy 1.14.5 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages\n",
      "Finished processing dependencies for dlr==1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'neo-ai-dlr' already exists and is not an empty directory.\n",
      "USING CUDA\n",
      "USING CUDNN\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "git clone --recursive https://github.com/neo-ai/neo-ai-dlr\n",
    "cd neo-ai-dlr \n",
    "mkdir -p build && cd build && cmake3 .. -DUSE_CUDA=ON -DUSE_CUDNN=ON && make -j\n",
    "cd ../python && python3 setup.py install --user && python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing requirements to dlr.egg-info/requires.txt\n",
      "writing dlr.egg-info/PKG-INFO\n",
      "writing top-level names to dlr.egg-info/top_level.txt\n",
      "writing dependency_links to dlr.egg-info/dependency_links.txt\n",
      "reading manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "writing manifest file 'dlr.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/__init__.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/api.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "copying build/lib/dlr/libpath.py -> build/bdist.linux-x86_64/egg/dlr\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/__init__.py to __init__.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/api.py to api.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dlr/libpath.py to libpath.pyc\n",
      "installing package data to build/bdist.linux-x86_64/egg\n",
      "running install_data\n",
      "copying ../lib/libdlr.so -> build/bdist.linux-x86_64/egg/dlr\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dlr.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/dlr-1.0-py2.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing dlr-1.0-py2.7.egg\n",
      "creating /home/ec2-user/.local/lib/python2.7/site-packages/dlr-1.0-py2.7.egg\n",
      "Extracting dlr-1.0-py2.7.egg to /home/ec2-user/.local/lib/python2.7/site-packages\n",
      "Adding dlr 1.0 to easy-install.pth file\n",
      "\n",
      "Installed /home/ec2-user/.local/lib/python2.7/site-packages/dlr-1.0-py2.7.egg\n",
      "Processing dependencies for dlr==1.0\n",
      "Searching for decorator==4.3.0\n",
      "Best match: decorator 4.3.0\n",
      "Adding decorator 4.3.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages\n",
      "Searching for numpy==1.14.5\n",
      "Best match: numpy 1.14.5\n",
      "Adding numpy 1.14.5 to easy-install.pth file\n",
      "\n",
      "Using /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages\n",
      "Finished processing dependencies for dlr==1.0\n"
     ]
    }
   ],
   "source": [
    "!cd neo-ai-dlr/python && python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from dlr import DLRModel\n",
    "\n",
    "imgMean = np.array([104, 117, 124], np.float)\n",
    "img = cv2.imread(\"TestImage.jpg\")\n",
    "img = cv2.resize(img.astype(float), (300, 300)) #resize\n",
    "img -= imgMean #subtract image mean\n",
    "img = img.reshape((1, 300, 300, 3))\n",
    "\n",
    "device = 'cpu'                           # Go, Raspberry Pi, go!\n",
    "model = DLRModel('neo_test', dev_type=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Placeholder']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_data = {'Placeholder': img}\n",
    "out = model.run(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
